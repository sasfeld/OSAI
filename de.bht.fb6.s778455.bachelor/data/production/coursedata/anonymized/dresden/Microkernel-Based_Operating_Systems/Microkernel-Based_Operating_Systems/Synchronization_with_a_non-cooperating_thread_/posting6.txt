ID: 490
CREATION_DATETIME: 1351029600000
MODIFICATION_DATETIME: 1351116000000
TITLE: To reiterate, let's h
PARENT_POSTING_ID: 479
POSTING_TYPE: answer
CONTENT:
To reiterate, let's have a look at the original problem: two (or more) threads are running in the same address space and access memory. As they <NAME_CORPUS_REPLACEMENT> be scheduled in parallel, two threads <NAME_CORPUS_REPLACEMENT> modify or read the data at the same time, which is a problem if a) both write a different value (write-write conflict) or b) one reads while the other one writes (read-write conflict). This is called a **race condition**.
The solution to this problem is to prevent the two threads from accessing memory in parallel. We <NAME_CORPUS_REPLACEMENT> do this globally and always only execute a single thread, but this idea is moot in the context of multicore systems. Instead, we limit such execution phases to areas that are wrapped with synchronization statements (usually something like `lock() /* critical section */ unlock()`.
The question now is how to implement these respective lock/unlock operations. And here you need some form of kernel support either in the form of kernel-level locks (e. g., <REMOVED_PERSON_ENTITY> futexes) or by having access to a mechanism that blocks additional threads from entering the critical section. The latter <NAME_CORPUS_REPLACEMENT> be implemented on  L4 using a dedicated serializer thread as shown by <REMOVED_PERSON_ENTITY> in yesterday's lecture.
Now there's an additional optimization: synchronization primitives are costly (because they involve the kernel) _and_ there <NAME_CORPUS_REPLACEMENT> be situations where only  a single thread tries to enter the critical sections and you don't want to pay the overhead in this case. Therefore, we add a counter variable that is modified atomically and counts how many threads try to access the critical section right now. Only if this counter tells us that there are multiple threads, they use the expensive blocking mechanism.
So far so good. What I understand from your question is that you want to prevent malicious threads from ignoring the counter and simply entering the critical section. This simply does not work: The thread only accesses memory. This memory is already mapped into the address space. There is no kernel / hardware feature to prevent a running thread from reading or writing to/from mapped memory and I don't think any privilege level modification will aid you to this end.
The whole thing (protecting all critical sections by using the proper locking primitives) is a software-level protocol problem and getting things wrong is simply a programming error from which no hardware mechanism will protect you. There are tools, though, that will help you find such situations. Valgrind's thread validators (helgrind, drd) come to mind. <REMOVED_PERSON_ENTITY>'s analysis works might be interesting as well (Google: RacerX and 'Bugs as Deviant Behavior')
TAGGED_CONTENT:
